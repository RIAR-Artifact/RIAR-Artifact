# Aritfact for "Program Reduction using Runtime Information"
## Introduction
Thank you for evaluating this artifact!

To evaluate this artifact, a Linux machine with docker installed is needed.
## List of Claims Supported by the Artifact
1. RIAR is more efficient than Perses, Vulcan, and C-Reduce. 
2. RIAR produces programs of sizes comparable to those generated by baseline tools.
3. Each type of runtime information is effective in reducing the overall time cost of program reduction.

## Setup
If docker is not installed, install it by following the [instructions](https://docs.docker.com/get-started/get-docker/).

Download riar_artifact

Change to the riar_artifact folder `cd ./riar_artifact`

Pull the docker image for running C and Java benchmarks and run a container from this image

`cd benchmark`

`./start_docker.sh`

`# You should be at /tmp after the above command finishes`

Note: This step might takes a while, mainly depending on the network bandwidth. It also takes up much disk space (nearly 100GB)
## Run Experiments
The experimental results may exhibit slight differences compared to those presented in the paper, primarily due to variations in experimental environments and the inherent randomness of the reduction process. However, these differences do not affect the overall comparisons, and the results still support the claims made in the paper.
### Claim 1 and Claim 2
- Context: The paper claims that RIAR is more efficient than Perses, Vulcan, and C-Reduce (in section 4.3) and RIAR produces programs of sizes comparable to those generated by baseline tools (in setion 4.4).
- Run Experiments in Benchmark-C
    - `cd /tmp/cenv`
    - `python3 scripts/run_rq12.py`
    - After the experiment finished, there should be `c_benchmarks_MFC` and `rq1_results` in `/tmp/cenv`, which contain the experiment result.
    - run `python3 scripts/count.py` and `python3 scripts/count.py c_benchmarks_MFC` to generate csv files of time and result size of the experiment.
- Run Experiments in Benchmark-Java
    - `cd /tmp/javaenv`
    - `python3 scripts/run_rq12.py`
    - After the experiment finished, there should be `java_benchmarks_MFC` and `rq1_results` in `/tmp/javaenv`, which contain the experiment result.
    - run `python3 scripts/count.py` and `python3 scripts/count.py java_benchmarks_MFC` to generate csv files of time and result size of the experiment.
    - If any processes remain running after the experiment concludes, they need to be manually terminated.
### Claim 3
- Context: The paper claims that each type of runtime information is effective in reducing the overall time cost of program reduction (in setion 4.5).
- Run Experiments in Benchmark-C
    - `cd /tmp/cenv`
    - `python3 scripts/run_rq3.py`
    - After the experiment finished, there should be `c_benchmarks_Blank`, `c_benchmarks_M`, `c_benchmarks_MF` and `c_benchmarks_MFC` in `/tmp/cenv`, which contain the experiment result.
    - run `python3 scripts/count.py c_benchmarks_Blank`, `python3 scripts/count.py c_benchmarks_M`, `python3 scripts/count.py c_benchmarks_MF` and `python3 scripts/count.py c_benchmarks_MFC` to generate csv files of time and result size of the experiment.
- Run Experiments in Benchmark-Java
    - `cd /tmp/javaenv`
    - `python3 scripts/run_rq3.py`
    - After the experiment finished, there should be `java_benchmarks_Blank` , `java_benchmarks_M`, `java_benchmarks_MF` and `java_benchmarks_MFC` in `/tmp/javaenv`, which contain the experiment result.
    - run `python3 scripts/count.py java_benchmarks_Blank`, `python3 scripts/count.py java_benchmarks_M`, `python3 scripts/count.py java_benchmarks_MF` and `python3 scripts/count.py java_benchmarks_MFC` to generate csv files of time and result size of the experiment.
